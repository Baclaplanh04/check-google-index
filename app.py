import streamlit as st
import pandas as pd
import requests
import time
import re
from io import BytesIO
from datetime import datetime, timedelta
from urllib.parse import quote_plus

st.set_page_config(page_title="Ki·ªÉm tra Google Index - Profile", layout="centered")

st.title("Ki·ªÉm tra Google Index cho danh s√°ch Profile")
st.markdown(
    "T·∫£i l√™n file `.xlsx` ch·ª©a danh s√°ch URL (b·∫Øt ƒë·∫ßu t·ª´ d√≤ng 3). ·ª®ng d·ª•ng s·∫Ω ki·ªÉm tra t·ª´ng URL xem c√≥ ƒë∆∞·ª£c Google index hay kh√¥ng, "
    "v√† (n·∫øu c√≥) l·∫•y ng√†y cached ƒë·ªÉ x√°c ƒë·ªãnh c√≥ index trong 30 ng√†y qua hay kh√¥ng."
)

# ====== UPLOAD FILE ======
uploaded_file = st.file_uploader("Ch·ªçn file Excel (.xlsx)", type=["xlsx"])

# ====== C√ÄI ƒê·∫∂T ======
st.sidebar.header("C√†i ƒë·∫∑t ki·ªÉm tra")
delay = st.sidebar.number_input("Delay gi·ªØa m·ªói request (gi√¢y)", min_value=0.5, value=2.0, step=0.5)
limit_urls = st.sidebar.number_input("Gi·ªõi h·∫°n t·ªëi ƒëa URLs (ƒë·ªÉ ch·∫°y 1 l·∫ßn)", min_value=1, value=1000, step=1)
user_agent = st.sidebar.selectbox("User-Agent m·∫´u", [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/118.0 Safari/537.36",
])

headers = {"User-Agent": user_agent}

# ====== H√ÄM CH√çNH ======
def is_indexed(url, headers):
    """Ki·ªÉm tra xem URL c√≥ ƒë∆∞·ª£c Google index hay kh√¥ng."""
    r = requests.get("https://www.google.com/search?q=" + quote_plus(url), headers=headers, timeout=20)
    text = r.text

    # N·∫øu c√≥ k·∫øt qu·∫£ th·ªëng k√™ s·ªë l∆∞·ª£ng -> c√≥ th·ªÉ ƒë√£ index
    if re.search(r"results?\s?\d|About [\d,]+ results|K·∫øt qu·∫£|C√≥ kho·∫£ng", text, re.I):
        return True, text

    # N·∫øu c√≥ th√¥ng b√°o "did not match any documents" -> ch∆∞a index
    if re.search(r"did not match any documents|No results found|Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£|kh√¥ng t√¨m th·∫•y", text, re.I):
        return False, text

    # N·∫øu c√≥ kh·ªëi k·∫øt qu·∫£ (class="g") -> c√≥ th·ªÉ index
    if 'class="g"' in text or 'id="search"' in text:
        return True, text

    # M·∫∑c ƒë·ªãnh l√† ch∆∞a index
    return False, text


def google_cache_date(url, headers):
    """D√πng cache:URL ƒë·ªÉ l·∫•y ng√†y cached page v√† parse date (n·∫øu c√≥)."""
    q = f"cache:{url}"
    url_cache = "https://www.google.com/search?q=" + quote_plus(q)
    r = requests.get(url_cache, headers=headers, timeout=20)
    text = r.text

    # T√¨m ng√†y th√°ng trong n·ªôi dung cache (d·∫°ng ti·∫øng Anh ho·∫∑c Vi·ªát)
    match = re.search(
        r"As it appeared on (\w+ \d{1,2}, \d{4})|L∆∞u trong b·ªô nh·ªõ cache.*?(\d{1,2}) th√°ng (\d{1,2}), (\d{4})",
        text,
        re.I
    )

    if match:
        try:
            if match.group(1):
                # English format
                return datetime.strptime(match.group(1), "%B %d, %Y").strftime("%d/%m/%Y")
            else:
                # Vietnamese format
                d, m, y = match.group(2), match.group(3), match.group(4)
                return f"{int(d):02d}/{int(m):02d}/{y}"
        except:
            return None
    return None


# ====== X·ª¨ L√ù FILE ======
if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file Excel: {e}")
        st.stop()

    # T√¨m c·ªôt ch·ª©a URL (t√™n g·∫ßn gi·ªëng 'profile', 'url', ho·∫∑c 'link')
    col_candidates = [c for c in df.columns if re.search(r'profile|url|link', c, re.I)]

    if not col_candidates:
        st.error("Kh√¥ng t√¨m th·∫•y c·ªôt ch·ª©a URL (v√≠ d·ª•: Profile, URL, Link). Vui l√≤ng ki·ªÉm tra l·∫°i file Excel.")
        st.stop()

    col_name = col_candidates[0]
    profiles = df[col_name].dropna().tolist()[:limit_urls]

    if not profiles:
        st.error("Kh√¥ng c√≥ URL n√†o trong file Excel.")
        st.stop()

    st.success(f"T√¨m th·∫•y {len(profiles)} URL. (S·∫Ω x·ª≠ l√Ω t·ªëi ƒëa {limit_urls} URL theo c√†i ƒë·∫∑t.)")

    # ====== CH·∫†Y KI·ªÇM TRA ======
    results = []
    progress = st.progress(0)
    status_text = st.empty()

    for i, url in enumerate(profiles):
        status_text.text(f"ƒêang ki·ªÉm tra {i+1}/{len(profiles)}: {url}")
        try:
            indexed, html = is_indexed(url, headers)
            cache_date = google_cache_date(url, headers) if indexed else None
            results.append({
                "URL": url,
                "ƒê√£ Index": "‚úÖ C√≥" if indexed else "‚ùå Kh√¥ng",
                "Ng√†y Cache": cache_date if cache_date else "-"
            })
        except Exception as e:
            results.append({"URL": url, "ƒê√£ Index": "‚ö†Ô∏è L·ªói", "Ng√†y Cache": str(e)})
        progress.progress((i + 1) / len(profiles))
        time.sleep(delay)

    # ====== HI·ªÇN TH·ªä K·∫æT QU·∫¢ ======
    st.subheader("K·∫øt qu·∫£ ki·ªÉm tra")
    result_df = pd.DataFrame(results)
    st.dataframe(result_df, use_container_width=True)

    # T·∫£i k·∫øt qu·∫£ Excel
    output = BytesIO()
    result_df.to_excel(output, index=False)
    st.download_button(
        label="üì• T·∫£i k·∫øt qu·∫£ v·ªÅ (.xlsx)",
        data=output.getvalue(),
        file_name="ket_qua_google_index.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

st.sidebar.markdown("---")
st.sidebar.info("·ª®ng d·ª•ng s·ª≠ d·ª•ng truy v·∫•n Google (mi·ªÖn ph√≠). N·∫øu b·∫°n mu·ªën k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n, c√¢n nh·∫Øc d√πng SerpAPI.")
