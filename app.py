import streamlit as st
import pandas as pd
import requests
import time
import re
from io import BytesIO
from datetime import datetime
from urllib.parse import quote_plus

# ======================
# C·∫§U H√åNH GIAO DI·ªÜN
# ======================
st.set_page_config(page_title="Ki·ªÉm tra Google Index - Profile", layout="centered")

st.title("üß≠ Ki·ªÉm tra Google Index cho danh s√°ch Profile")
st.markdown("""
T·∫£i l√™n file **.xlsx** ch·ª©a c·ªôt **Profile** (b·∫Øt ƒë·∫ßu t·ª´ d√≤ng 3).  
·ª®ng d·ª•ng s·∫Ω ki·ªÉm tra t·ª´ng URL xem c√≥ ƒë∆∞·ª£c Google index hay kh√¥ng v√† (n·∫øu c√≥) l·∫•y ng√†y cached ƒë·ªÉ x√°c ƒë·ªãnh c√≥ index trong 30 ng√†y qua hay kh√¥ng.
""")

# ======================
# C√ÄI ƒê·∫∂T NG∆Ø·ªúI D√ôNG
# ======================
delay = st.sidebar.number_input("‚è±Ô∏è Delay gi·ªØa m·ªói request (gi√¢y)", min_value=1.0, max_value=10.0, value=2.0, step=0.5)
limit = st.sidebar.number_input("üî¢ Gi·ªõi h·∫°n t·ªëi ƒëa URLs (ƒë·ªÉ ch·∫°y 1 l·∫ßn)", min_value=1, max_value=1000, value=1000, step=1)
user_agent = st.sidebar.selectbox("üß© User-Agent m·∫´u", [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)"
])

st.sidebar.info("·ª®ng d·ª•ng s·ª≠ d·ª•ng truy v·∫•n Google (mi·ªÖn ph√≠). N·∫øu b·∫°n mu·ªën k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n, c·∫ßn c√¢n nh·∫Øc d√πng SerpAPI.")


# ======================
# H√ÄM KI·ªÇM TRA INDEX
# ======================
def is_indexed(url, headers):
    """Ki·ªÉm tra xem URL c√≥ ƒë∆∞·ª£c Google index hay kh√¥ng."""
    r = requests.get("https://www.google.com/search?q=" + quote_plus(url), headers=headers, timeout=20)
    text = r.text

    # N·∫øu c√≥ k·∫øt qu·∫£ th·ªëng k√™ s·ªë l∆∞·ª£ng -> c√≥ th·ªÉ ƒë√£ index
    if re.search(r"results?\s?\d|About [\d,]+ results|K·∫øt qu·∫£|C√≥ kho·∫£ng", text, re.I):
        return True, text

    # N·∫øu c√≥ th√¥ng b√°o "did not match any documents" -> ch∆∞a index
    if re.search(r"did not match any documents|No results found|Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£|kh√¥ng t√¨m th·∫•y", text, re.I):
        return False, text

    # N·∫øu c√≥ kh·ªëi k·∫øt qu·∫£ (class="g") -> c√≥ th·ªÉ index
    if 'class="g"' in text or 'id="search"' in text:
        return True, text

    # M·∫∑c ƒë·ªãnh l√† ch∆∞a index
    return False, text


# ======================
# H√ÄM L·∫§Y NG√ÄY CACHE
# ======================
def google_cache_date(url, headers):
    """D√πng cache:URL ƒë·ªÉ l·∫•y ng√†y cached page v√† parse date (n·∫øu c√≥)."""
    q = f"cache:{url}"
    url_cache = "https://www.google.com/search?q=" + quote_plus(q)
    r = requests.get(url_cache, headers=headers, timeout=20)
    text = r.text

    # T√¨m ng√†y th√°ng trong n·ªôi dung cache (d·∫°ng ti·∫øng Anh ho·∫∑c Vi·ªát)
    match = re.search(
        r"As it appeared on (\w+ \d{1,2}, \d{4})|L∆∞u trong b·ªô nh·ªõ cache.*?(\d{1,2}) th√°ng (\d{1,2}), (\d{4})",
        text,
        re.I
    )

    if match:
        try:
            if match.group(1):
                # English format
                return datetime.strptime(match.group(1), "%B %d, %Y").strftime("%d/%m/%Y")
            else:
                # Vietnamese format
                d, m, y = match.group(2), match.group(3), match.group(4)
                return f"{int(d):02d}/{int(m):02d}/{y}"
        except:
            return None

    return None


# ======================
# X·ª¨ L√ù FILE NG∆Ø·ªúI D√ôNG
# ======================
uploaded_file = st.file_uploader("üìÇ Ch·ªçn file Excel (.xlsx)", type=["xlsx"])

if uploaded_file:
    try:
        df = pd.read_excel(uploaded_file)
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file Excel: {e}")
        st.stop()

    if 'Profile' not in df.columns:
        st.error("File Excel ph·∫£i c√≥ c·ªôt 'Profile' ch·ª©a danh s√°ch URL c·∫ßn ki·ªÉm tra.")
        st.stop()

    profiles = df['Profile'].dropna().tolist()
    profiles = profiles[:limit]

    st.success(f"T√¨m th·∫•y {len(profiles)} URL. (S·∫Ω x·ª≠ l√Ω t·ªëi ƒëa {limit} URL theo c√†i ƒë·∫∑t.)")

    if st.button("üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra"):
        headers = {"User-Agent": user_agent}
        results = []

        progress = st.progress(0)
        status_text = st.empty()

        for i, url in enumerate(profiles, start=1):
            status_text.text(f"ƒêang ki·ªÉm tra {i}/{len(profiles)}: {url}")

            try:
                indexed, text = is_indexed(url, headers)
                cached_date = google_cache_date(url, headers) if indexed else None

                results.append({
                    "URL": url,
                    "ƒê√£ index": "‚úÖ C√≥" if indexed else "‚ùå Kh√¥ng",
                    "Ng√†y cache": cached_date if cached_date else "",
                })

            except Exception as e:
                results.append({
                    "URL": url,
                    "ƒê√£ index": "‚ö†Ô∏è L·ªói",
                    "Ng√†y cache": str(e),
                })

            progress.progress(i / len(profiles))
            time.sleep(delay)

        st.success("üéâ Ho√†n t·∫•t ki·ªÉm tra!")
        result_df = pd.DataFrame(results)

        # Xu·∫•t k·∫øt qu·∫£
        output = BytesIO()
        result_df.to_excel(output, index=False)
        st.download_button("üì• T·∫£i k·∫øt qu·∫£ Excel", data=output.getvalue(), file_name="indexed_results.xlsx")
